{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this part of the analysis, we focused on reassigning individuals into the identified clusters using illustrative variables. Building on the clustering results from the first step, we evaluated two algorithms, Random Forest and Gradient Boosting.\n",
    "Here, we focus on the segmentation of the green variables.\n",
    "In the first part, using the orange variables and then using a specific set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"fic_epita_kantar_codes.csv\"\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Define Orange and Green variable groups\n",
    "orange_vars = [\n",
    "    \"A9_1_slice\", \"A9_2_slice\", \"A9_3_slice\", \"A9_4_slice\", \"A9_5_slice\",\n",
    "    \"A9_6_slice\", \"A9_7_slice\", \"A9_8_slice\", \"A9_9_slice\", \"A9_10_slice\",\n",
    "    \"A9_11_slice\", \"A9_12_slice\", \"A9_13_slice\", \"A9_14_slice\", \"A9_15_slice\",\n",
    "    \"A9_16_slice\", \"A10_1_slice\", \"A10_2_slice\", \"A10_3_slice\", \"A10_4_slice\",\n",
    "    \"A10_5_slice\", \"A10_6_slice\", \"A10_7_slice\", \"A10_8_slice\",\n",
    "    \"A11_1_slice\", \"A11_2_slice\", \"A11_3_slice\", \"A11_4_slice\", \"A11_5_slice\",\n",
    "    \"A11_6_slice\", \"A11_7_slice\", \"A11_8_slice\", \"A11_9_slice\", \"A11_10_slice\",\n",
    "    \"A11_11_slice\", \"A11_12_slice\", \"A11_13_slice\"\n",
    "]\n",
    "green_vars = [\n",
    "    \"A11_1_slice\", \"A12\", \"A13\", \"A14\", \"A4\", \"A5\", \"A5bis\", \"A8_1_slice\",\n",
    "    \"A8_2_slice\", \"A8_3_slice\", \"A8_4_slice\", \"B1_1_slice\", \"B1_2_slice\",\n",
    "    \"B2_1_slice\", \"B2_2_slice\", \"B3\", \"B4\", \"B6\", \"C1_1_slice\", \"C1_2_slice\",\n",
    "    \"C1_3_slice\", \"C1_4_slice\", \"C1_5_slice\", \"C1_6_slice\", \"C1_7_slice\",\n",
    "    \"C1_8_slice\", \"C1_9_slice\"\n",
    "]\n",
    "specific_vars = [\n",
    "    \"rs3\", \"rs5\", \"rs6\", \"RS1\", \"RS191\", \"RS192\", \"RS193\", \"RS102RECAP\",\n",
    "    \"rs11recap2\", \"RS11recap\", \"RS193bis\", \"RS2Recap\", \"RS56Recap\",\n",
    "    \"RS2\", \"RS11\", \"RS102\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the cluster column using the best algo with the best number of cluster found in part 1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in Green variables\n",
    "green_data = data[green_vars].fillna(0)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_green_data = scaler.fit_transform(green_data)\n",
    "\n",
    "# Perform KMeans clustering with 4 clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "data['cluster_green'] = kmeans.fit_predict(scaled_green_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation based on green variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Green Variables) with Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.07      0.11       103\n",
      "           1       0.56      0.28      0.37       256\n",
      "           2       0.48      0.40      0.44       100\n",
      "           3       0.60      0.85      0.71       541\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.49      0.40      0.41      1000\n",
      "weighted avg       0.55      0.58      0.53      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features (Green variables) and target (Orange clusters)\n",
    "X_orange = data[orange_vars].fillna(0)\n",
    "y_green = data['cluster_green']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_orange, X_test_orange, y_train, y_test = train_test_split(\n",
    "    X_orange, y_green, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and Evaluate model\n",
    "rf_orange = RandomForestClassifier(random_state=42)\n",
    "rf_orange.fit(X_train_orange, y_train)\n",
    "\n",
    "y_pred_orange = rf_orange.predict(X_test_orange)\n",
    "print(\"Classification Report (Green Variables) with Random Forest:\\n\", classification_report(y_test, y_pred_orange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Specific Variables with Gradient Boosting):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.18      0.26       103\n",
      "           1       0.60      0.30      0.40       256\n",
      "           2       0.47      0.40      0.43       100\n",
      "           3       0.61      0.84      0.70       541\n",
      "\n",
      "    accuracy                           0.59      1000\n",
      "   macro avg       0.53      0.43      0.45      1000\n",
      "weighted avg       0.58      0.59      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features (Orange variables) and target (Green clusters)\n",
    "X2 = data[orange_vars].fillna(0)\n",
    "y2 =  data['cluster_green']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2, y2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and Evaluate model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train2, y_train2)\n",
    "y_pred2 = gb.predict(X_test2)\n",
    "print(\"Classification Report (Specific Variables with Gradient Boosting):\\n\", classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing results to try to improve the average performance we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best Cross-Validation Accuracy: 0.6073\n",
      "Classification Report (Green Variables) with Random Forest Optimization:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.11      0.18       103\n",
      "           1       0.61      0.26      0.37       256\n",
      "           2       0.42      0.33      0.37       100\n",
      "           3       0.60      0.87      0.71       541\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.55      0.39      0.41      1000\n",
      "weighted avg       0.58      0.58      0.53      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features (Green variables) and target (Orange clusters)\n",
    "X_orange = data[orange_vars].fillna(0)\n",
    "y_green = data['cluster_green']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_orange, X_test_orange, y_train, y_test = train_test_split(\n",
    "    X_orange, y_green, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV to optimize hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=3, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV on training data\n",
    "grid_search.fit(X_train_orange, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Train and Evaluate model with optimized hyperparameters\n",
    "rf_optimized = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_optimized.fit(X_train_orange, y_train)\n",
    "y_pred_orange_optimized = rf_optimized.predict(X_test_orange)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report (Green Variables) with Random Forest Optimization:\\n\",\n",
    "      classification_report(y_test, y_pred_orange_optimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (Gradient Boosting): {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation Accuracy (Gradient Boosting): 0.6070\n",
      "Classification Report (Specific Variables with Gradient Boosting Optimization):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.17      0.24       103\n",
      "           1       0.61      0.31      0.41       256\n",
      "           2       0.44      0.35      0.39       100\n",
      "           3       0.61      0.84      0.71       541\n",
      "\n",
      "    accuracy                           0.59      1000\n",
      "   macro avg       0.52      0.42      0.44      1000\n",
      "weighted avg       0.57      0.59      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features (Orange variables) and target (Green clusters)\n",
    "X2 = data[orange_vars].fillna(0)\n",
    "y2 = data['cluster_green']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2, y2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for Gradient Boosting\n",
    "param_grid2 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize a Gradient Boosting classifier\n",
    "gb2 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV to optimize hyperparameters\n",
    "grid_search2 = GridSearchCV(estimator=gb2, param_grid=param_grid2, \n",
    "                            scoring='accuracy', cv=3, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV on training data\n",
    "grid_search2.fit(X_train2, y_train2)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params2 = grid_search2.best_params_\n",
    "best_score2 = grid_search2.best_score_\n",
    "print(f\"Best Hyperparameters (Gradient Boosting): {best_params2}\")\n",
    "print(f\"Best Cross-Validation Accuracy (Gradient Boosting): {best_score2:.4f}\")\n",
    "\n",
    "# Train and Evaluate model with optimized hyperparameters\n",
    "gb_optimized = GradientBoostingClassifier(**best_params2, random_state=42)\n",
    "gb_optimized.fit(X_train2, y_train2)\n",
    "y_pred2_optimized = gb_optimized.predict(X_test2)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report (Specific Variables with Gradient Boosting Optimization):\\n\",\n",
    "      classification_report(y_test2, y_pred2_optimized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation based on specific variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Specific Variables) with Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.34      0.27       103\n",
      "           1       0.64      0.67      0.65       256\n",
      "           2       0.18      0.10      0.13       100\n",
      "           3       0.67      0.64      0.66       541\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.43      0.44      0.43      1000\n",
      "weighted avg       0.57      0.56      0.56      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Random Forest\n",
    "# Extract features (Specific variables) and target (Green clusters)\n",
    "X_specific = data[specific_vars].fillna(0)\n",
    "y_green = data['cluster_green']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_specific, X_test_specific, y_train, y_test = train_test_split(\n",
    "    X_specific, y_green, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Convert weights into a dictionary\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Train and Evaluate model\n",
    "rf_specific = RandomForestClassifier(random_state=42, class_weight=class_weights_dict)\n",
    "rf_specific.fit(X_train_specific, y_train)\n",
    "y_pred_specific = rf_specific.predict(X_test_specific)\n",
    "print(\"Classification Report (Specific Variables) with Random Forest:\\n\", classification_report(y_test, y_pred_specific))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Specific Variables) with Gradient Boosting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.01      0.02       103\n",
      "           1       0.74      0.69      0.71       256\n",
      "           2       0.10      0.01      0.02       100\n",
      "           3       0.67      0.92      0.77       541\n",
      "\n",
      "    accuracy                           0.68      1000\n",
      "   macro avg       0.41      0.41      0.38      1000\n",
      "weighted avg       0.58      0.68      0.61      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Gradient Boosting\n",
    "# Extract features (Specific variables) and target (Green clusters)\n",
    "X_specific = data[specific_vars].fillna(0)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_specific, X_test_specific, y_train, y_test = train_test_split(\n",
    "    X_specific, y_green, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and Evaluate model\n",
    "gb_specific = GradientBoostingClassifier(random_state=42)\n",
    "gb_specific.fit(X_train_specific, y_train)\n",
    "y_pred_specific = gb_specific.predict(X_test_specific)\n",
    "print(\"Classification Report (Specific Variables) with Gradient Boosting:\\n\", classification_report(y_test, y_pred_specific))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing results to try to improve the average performance we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.6718\n",
      "Classification Report (Specific Variables with Hyperparameter Optimization):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       103\n",
      "           1       0.71      0.71      0.71       256\n",
      "           2       0.00      0.00      0.00       100\n",
      "           3       0.68      0.93      0.78       541\n",
      "\n",
      "    accuracy                           0.68      1000\n",
      "   macro avg       0.35      0.41      0.37      1000\n",
      "weighted avg       0.55      0.68      0.60      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floflo/Documents/epita/epita-ml-scia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/floflo/Documents/epita/epita-ml-scia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/floflo/Documents/epita/epita-ml-scia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# For Random Forest\n",
    "# Extract features (Specific variables) and target (Green clusters)\n",
    "X_specific = data[specific_vars].fillna(0)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_specific, X_test_specific, y_train, y_test = train_test_split(\n",
    "    X_specific, y_green, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV to optimize hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=3, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV on training data\n",
    "grid_search.fit(X_train_specific, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Train and Evaluate model with optimized hyperparameters\n",
    "rf_optimized = RandomForestClassifier(**best_params, random_state=42)\n",
    "rf_optimized.fit(X_train_specific, y_train)\n",
    "y_pred_specific_optimized = rf_optimized.predict(X_test_specific)\n",
    "print(\"Classification Report (Specific Variables with Hyperparameter Optimization):\\n\",\n",
    "      classification_report(y_test, y_pred_specific_optimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (Gradient Boosting): {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation Accuracy (Gradient Boosting): 0.6788\n",
      "Classification Report (Specific Variables with Gradient Boosting Optimization):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       103\n",
      "           1       0.75      0.69      0.72       256\n",
      "           2       0.00      0.00      0.00       100\n",
      "           3       0.67      0.94      0.78       541\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.35      0.41      0.37      1000\n",
      "weighted avg       0.55      0.69      0.61      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floflo/Documents/epita/epita-ml-scia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/floflo/Documents/epita/epita-ml-scia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/floflo/Documents/epita/epita-ml-scia/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# For Gradient Boosting\n",
    "# Define parameter grid for Gradient Boosting\n",
    "param_grid2 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize a Gradient Boosting classifier\n",
    "gb2 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV to optimize hyperparameters\n",
    "grid_search2 = GridSearchCV(estimator=gb2, param_grid=param_grid2, \n",
    "                            scoring='accuracy', cv=3, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV on the training data (using y_train instead of y_green)\n",
    "grid_search2.fit(X_train_specific, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params2 = grid_search2.best_params_\n",
    "best_score2 = grid_search2.best_score_\n",
    "print(f\"Best Hyperparameters (Gradient Boosting): {best_params2}\")\n",
    "print(f\"Best Cross-Validation Accuracy (Gradient Boosting): {best_score2:.4f}\")\n",
    "\n",
    "# Train and Evaluate model with optimized hyperparameters\n",
    "gb_optimized = GradientBoostingClassifier(**best_params2, random_state=42)\n",
    "gb_optimized.fit(X_train_specific, y_train)\n",
    "y_pred_specific_optimized = gb_optimized.predict(X_test_specific)\n",
    "print(\"Classification Report (Specific Variables with Gradient Boosting Optimization):\\n\",\n",
    "      classification_report(y_test, y_pred_specific_optimized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this part of the analysis, we reassigned individuals to their respective clusters using the illustrative variables derived from the Green segmentation. The segmentation yielded pretty average performance when using either the Green variables or the specific given variables. Optimizing hyperparameters did not significantly improve the results, with accuracies of approximately 0.6 for the Green variables and 0.7 for the specific given variables.\n",
    "\n",
    "Both Random Forest and Gradient Boosting produced similar results, with a difference of only 0.01 in accuracy in each case. Given this negligible difference and the fact that Random Forest is a faster algorithm, it is the recommended choice, as the slight performance improvement offered by Gradient Boosting does not justify the additional computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
